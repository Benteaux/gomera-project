{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtVMqQQGWsMWVCowsJz8L3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benteaux/gomera-project/blob/main/awsr_silbogomero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies and Such**"
      ],
      "metadata": {
        "id": "v_8TERujn4w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install tensorflow-io"
      ],
      "metadata": {
        "id": "4TPq1aYMHZqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIRP6zmiDxjC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming you've stored your data in a google drive setup like mine\n",
        "drive.mount(\"/content/drive\")\n",
        "MAIN = os.path.join(\"/content\", \"drive\", \"My Drive\", \"silboData\")\n",
        "\n",
        "# note that, to be used with tensorflow, the data must be converted to the PCM 16 subtype. This can be done with the soundfile library.\n",
        "wordAudio = tf.data.Dataset.list_files(MAIN + \"/words/clips/*-16.wav\")\n",
        "wordText = tf.data.Dataset.list_files(MAIN + \"/words/transcriptions/*.txt\")"
      ],
      "metadata": {
        "id": "uhrgTKwfHRH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Sorting and Preprocessing**"
      ],
      "metadata": {
        "id": "vxPn6-kWn7X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "waFiles = []\n",
        "for element in wordAudio.as_numpy_iterator():\n",
        "  waFiles.append(element)"
      ],
      "metadata": {
        "id": "wiPSFHmuxeCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audiofiles = sorted(set(waFiles))"
      ],
      "metadata": {
        "id": "-8D8PW-vlM7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "wordFiles = []\n",
        "for element in wordText.as_numpy_iterator():\n",
        "  wordFiles.append(element)\n",
        "numbers1 = [int(re.search(r'\\d+', filepath.decode()).group()) for filepath in wordFiles]\n"
      ],
      "metadata": {
        "id": "x2WqBlv6ya9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textfiles = [textfile for _, textfile in (sorted(zip(numbers1, wordFiles)))]"
      ],
      "metadata": {
        "id": "Jh4YWa6LyTdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(textfiles) == len(audiofiles)"
      ],
      "metadata": {
        "id": "S-xsXkg8qQjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**audio preprocessing functions**"
      ],
      "metadata": {
        "id": "FzenqUm0xS-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assuming usage of both sentence data and single word data from the dataset, 2651682 was determined to be the largest shape of any audio file.\n",
        "# 44384 is the median, 87053 is the mean, and 8939 is the min\n",
        "def load_mono_pad(filepath):\n",
        "    audio = tf.io.read_file(filepath)\n",
        "    audio, sampling = tf.audio.decode_wav(audio, desired_channels = 1)\n",
        "    sampling = tf.cast(sampling, dtype = tf.int64)\n",
        "    audio = tf.squeeze(audio, axis = -1)\n",
        "    size = # longest audio length being accepted. note that if you choose to cut stuff off, you may need to preprocess the text differently\n",
        "    audio = audio[:size]\n",
        "    padding = tf.zeros([size] - tf.shape(audio), dtype = tf.float32)\n",
        "    audio = tf.concat([audio, padding], axis = 0)\n",
        "    # audio and sample are tf.float32\n",
        "    return audio\n",
        "\n"
      ],
      "metadata": {
        "id": "btKRoBU-H4-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get mel spectrogram and scale to dB\n",
        "sampling = 44100 # can also return sampling rate from load_mono_pad if desired\n",
        "def log_mel_features(audio):\n",
        "  # window = 25ms, stride = 10ms, 512 nffts was taken from TF documentation\n",
        "  spect = tfio.audio.spectrogram(audio, 512, 25, 10)\n",
        "\n",
        "  # 80 dimensional mel features as specified by referenced paper\n",
        "  mel = tfio.audio.melscale(spect, sampling, mels = 80, fmin = 0, fmax = 16000)\n",
        "\n",
        "  # scale to dB. 80 taken from TF documentation\n",
        "  mel = tfio.audio.dbscale(mel, top_db = 80)\n",
        "\n",
        "  return mel"
      ],
      "metadata": {
        "id": "mxqyh5xQIpfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text preprocessing functions**"
      ],
      "metadata": {
        "id": "odV967jPfxwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(filepath):\n",
        "  def actual_stuff(filepath):\n",
        "    text = tf.io.read_file(filepath).numpy()\n",
        "    return text\n",
        "\n",
        "  text = tf.py_function(actual_stuff, [filepath], tf.string)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "LRgW0a9c25cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note that spanish characters like 치, 침, etc, are converted to English equivalents (치 -> a, 침 -> n)\n",
        "def text_process(text):\n",
        "    def replace_chars(text_tensor):\n",
        "        normal_text = tf_text.normalize_utf8(text_tensor, 'NFKD')\n",
        "        normal_text = tf.strings.lower(normal_text)\n",
        "        normal_text = tf.strings.regex_replace(normal_text, '[^ a-z]', '')\n",
        "        normal_text = tf.strings.regex_replace(normal_text, '(.)', r'\\1 ') # character-level tokenization\n",
        "        normal_text = tf.strings.strip(normal_text)\n",
        "        normal_text = tf.strings.join(['[START]', normal_text, \"[END]\"], separator = ' ')\n",
        "        return normal_text\n",
        "\n",
        "    processed_texts = replace_chars(text)\n",
        "\n",
        "    return processed_texts\n"
      ],
      "metadata": {
        "id": "_J-Bku70IJeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(audio, text):\n",
        "  audio = log_mel_features(audio)\n",
        "  text = vectorizer(text)\n",
        "  textIn = text[:, :-1]\n",
        "  textOut = text[:, 1:]\n",
        "  return (audio, textIn), textOut"
      ],
      "metadata": {
        "id": "jgaWxQNr0ZkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textData = tf.data.TextLineDataset(textfiles)\n",
        "audioData = tf.data.Dataset.from_tensor_slices(audiofiles)\n",
        "audioData = audioData.map(load_mono_pad)\n",
        "data = tf.data.Dataset.zip(audioData, textData)\n",
        "data = data.batch(16, drop_remainder = True)"
      ],
      "metadata": {
        "id": "U7NWcPEu75aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = 30\n",
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens = vocabulary, standardize = text_process, output_sequence_length = 30)"
      ],
      "metadata": {
        "id": "ZAFSl_07T1sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.adapt(['[START]',\n",
        " '[END]',\n",
        " 'a',\n",
        " 'b',\n",
        " 'c',\n",
        " 'd',\n",
        " 'e',\n",
        " 'f',\n",
        " 'g',\n",
        " 'h',\n",
        " 'i',\n",
        " 'j',\n",
        " 'k',\n",
        " 'l',\n",
        " 'm',\n",
        " 'n',\n",
        " 'o',\n",
        " 'p',\n",
        " 'q',\n",
        " 'r',\n",
        " 's',\n",
        " 't',\n",
        " 'u',\n",
        " 'v',\n",
        " 'w',\n",
        " 'x',\n",
        " 'y',\n",
        " 'z'])"
      ],
      "metadata": {
        "id": "R8c3SVqZo33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer = len(textfiles)\n",
        "data = data.map(preprocess)\n",
        "data = data.shuffle(buffer)\n",
        "data = data.shuffle(buffer)"
      ],
      "metadata": {
        "id": "QKHbctQHuwxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = # of batches in your dataset. Depends on how much of the silbo gomero data you use\n",
        "eighty = int(size* 0.8)\n",
        "train_ds = data.take(eighty)\n",
        "test_ds = data.skip(eighty).take(size - eighty)"
      ],
      "metadata": {
        "id": "4Q8R1e7Pw3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "b6onFkgbp2ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modelled after the referenced paper's shared encoder\n",
        "from tensorflow.keras.layers import GRU, Dense, MaxPooling1D\n",
        "from tensorflow import keras\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "\n",
        "  def __init__(self, lstm_units = 512, proj_units = 160, embed = 32):\n",
        "    super().__init__()\n",
        "    self.downsampling = tf.keras.layers.MaxPooling1D(pool_size = (3))\n",
        "    self.rnn1 = GRU(lstm_units, return_sequences = True)\n",
        "    self.proj1 = Dense(proj_units, activation = 'relu')\n",
        "    self.rnn2 = GRU(lstm_units, return_sequences = True)\n",
        "    self.reduction = MaxPooling1D(pool_size = (2))\n",
        "    self.proj2 = Dense(proj_units, activation = 'relu')\n",
        "    self.rnn3 = GRU(lstm_units, return_sequences = True)\n",
        "    self.proj3 = Dense(proj_units, activation = 'relu')\n",
        "\n",
        "  def call(self, audio):\n",
        "    x = self.downsampling(audio)\n",
        "    x = self.rnn1(x)\n",
        "    x = self.proj1(x)\n",
        "    x = self.rnn2(x)\n",
        "    x = self.reduction(x)\n",
        "    x = self.proj2(x)\n",
        "    x = self.rnn3(x)\n",
        "    x = self.proj3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "BE7vSru6QMQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Encoder.add_method\n",
        "def load_mono_pad(self, filepath):\n",
        "    audio = tf.io.read_file(filepath)\n",
        "    audio, sampling = tf.audio.decode_wav(audio, desired_channels = 1)\n",
        "    audio = tf.squeeze(audio, axis = -1)\n",
        "    size = # same size as before\n",
        "    audio = audio[:size]\n",
        "    padding = tf.zeros([size] - tf.shape(audio), dtype = tf.float32)\n",
        "    audio = tf.concat([audio, padding], axis = 0)\n",
        "\n",
        "    return audio, sampling"
      ],
      "metadata": {
        "id": "uSytR4NKVVM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Encoder.add_method\n",
        "def log_mel_features(self, audio, sampling):\n",
        "    spect = tfio.audio.spectrogram(audio, 512, 25, 10)\n",
        "    mel = tfio.audio.melscale(spect, sampling, mels = 80, fmin = 0, fmax = 8000)\n",
        "    mel = tfio.audio.dbscale(mel, top_db = 80)\n",
        "\n",
        "    return mel"
      ],
      "metadata": {
        "id": "fT1pSUoUVToA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Encoder.add_method\n",
        "def convert_inputs(self, filepath):\n",
        "    audio = self.log_mel_features(*(self.load_mono_pad(filepath)))\n",
        "    audio = tf.expand_dims(audio, axis = 0)\n",
        "    context = self(audio)\n",
        "    return context"
      ],
      "metadata": {
        "id": "p_oKJGTfVNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelled after the referenced paper\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads = 2):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim = 64, num_heads = num_heads)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "  def call(self, x, context):\n",
        "    attention = self.mha(query = x, value = context)\n",
        "    x = self.add([x, attention])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6MZM0nHib5x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelled after the referenced paper\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, lstm_units = 512, proj_units = 160, embed = 24, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab_size = tokenizer.vocabulary_size()\n",
        "    self.id_to_text = tf.keras.layers.StringLookup(vocabulary = tokenizer.get_vocabulary(), mask_token = '', oov_token = '[UNK]', invert = True)\n",
        "    self.text_to_id = tf.keras.layers.StringLookup(vocabulary = tokenizer.get_vocabulary(), mask_token = '', oov_token = '[UNK]')\n",
        "    self.start_token = self.text_to_id('[START]')\n",
        "    self.end_token = self.text_to_id('[END]')\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, embed)\n",
        "    self.cross = CrossAttention()\n",
        "    self.rnn1 = GRU(units = lstm_units, return_sequences = True, return_state = True)\n",
        "    self.rnn2 = GRU(units = lstm_units, return_sequences = True, return_state = True)\n",
        "    self.proj = Dense(units = proj_units, activation = 'relu')\n",
        "    self.out = Dense(units = self.vocab_size)\n",
        "\n",
        "\n",
        "  def call(self, context, x, state1 = None, state2 = None, return_states = False):\n",
        "    x = self.embedding(x)\n",
        "    x = self.cross(x, context)\n",
        "    x, state1 = self.rnn1(x, initial_state = state1)\n",
        "    x, state2 = self.rnn2(x, initial_state = state2)\n",
        "    x = self.proj(x)\n",
        "    logits = self.out(x)\n",
        "    if return_states:\n",
        "      return logits, state1, state2\n",
        "    else:\n",
        "      return logits"
      ],
      "metadata": {
        "id": "sjWd6eCPbywd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
        "  embeddings = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn1.get_initial_state(embeddings)[0], self.rnn2.get_initial_state(embeddings)[0]"
      ],
      "metadata": {
        "id": "kL9zQONUr_Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, x):\n",
        "  text = self.id_to_text(x)\n",
        "  text = tf.strings.reduce_join(text, axis = -1, separator = ' ')\n",
        "  text = tf.strings.regex_replace(text, '^ *\\[START\\] *', '')\n",
        "  text = tf.strings.regex_replace(text, ' *\\[END\\] *$', '')\n",
        "  return text"
      ],
      "metadata": {
        "id": "3FhTl9nnEZ7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state1, state2, temperature = 0.0):\n",
        "  logits, state1, state2 = self(context, next_token, state1, state2, return_states = True)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis = -1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples = 1)\n",
        "  done = done | (next_token == self.end_token)\n",
        "  next_token = tf.where(done, tf.constant(0, dtype = tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state1, state2"
      ],
      "metadata": {
        "id": "-ee0IAE2FcBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Whistler(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    encoder = Encoder()\n",
        "    decoder = Decoder(tokenizer = self.tokenizer)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, input):\n",
        "    context, x = input\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "veQjb8d7NtTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Whistler.add_method\n",
        "def translate(self, filepath, *, max_length = 50, temperature = 0.0):\n",
        "  context = self.encoder.convert_inputs(filepath)\n",
        "\n",
        "  tokens = []\n",
        "  next_token, done, state1, state2 = self.decoder.get_initial_state(context)\n",
        "  for _ in range(max_length):\n",
        "    next_token, done, state1, state2 = self.decoder.get_next_token(context, next_token, done, state1, state2, temperature)\n",
        "    tokens.append(next_token)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  tokens = tf.concat(tokens, axis = -1)\n",
        "  output = self.decoder.tokens_to_text(tokens)\n",
        "  return output"
      ],
      "metadata": {
        "id": "T6C37Uu3JJW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits = True, reduction = 'none'\n",
        "  )\n",
        "  loss = loss_fn(y_true, y_pred)\n",
        "  mask = tf.cast(y_true != 0, loss.dtype)\n",
        "\n",
        "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "3CcMczrLXvmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_accuracy(y_true, y_pred):\n",
        "  y_pred = tf.argmax(y_pred, axis = -1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "  masked = tf.cast(y_true != 0, tf.float32)\n",
        "  matched = tf.cast(y_true == y_pred, tf.float32) * masked\n",
        "\n",
        "  return tf.reduce_sum(matched) / tf.reduce_sum(masked)"
      ],
      "metadata": {
        "id": "njvDCzMZZ7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "UNITS = 256"
      ],
      "metadata": {
        "id": "hWDdIIZcPL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Whistler(vectorizer)\n",
        "opt = tf.keras.optimizers.Adam(1e-4)\n",
        "model.compile(optimizer = opt, loss = masked_loss, metrics = [masked_loss, masked_accuracy])"
      ],
      "metadata": {
        "id": "O07j6w5JanR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training & Testing**"
      ],
      "metadata": {
        "id": "o5GOQDkwpwQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.translate(audiofiles[1025])"
      ],
      "metadata": {
        "id": "5zebG3Nolkrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds.repeat(), steps = 20, return_dict = True)"
      ],
      "metadata": {
        "id": "R94u2STlawLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    epochs = epochs,\n",
        "    steps_per_epoch = 100\n",
        ")"
      ],
      "metadata": {
        "id": "I8kf9vG-a3ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds.repeat(), steps = 20, return_dict = True)"
      ],
      "metadata": {
        "id": "vPJL8f8lqK6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference Test**"
      ],
      "metadata": {
        "id": "JU0ja8STqB76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp = audiofiles[41]\n",
        "fp"
      ],
      "metadata": {
        "id": "CRnPP4-_viyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.translate(fp)"
      ],
      "metadata": {
        "id": "sAP7Qm5tUFh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_text(textfiles[41])"
      ],
      "metadata": {
        "id": "6RPRU2d6qISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should output \"farmacia\"\n",
        "output.numpy()[0].decode()"
      ],
      "metadata": {
        "id": "zOC9qE11UGAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10, 20):\n",
        "  print(f'Prediction: {model.translate(audiofiles[i]).numpy()[0].decode()}')\n",
        "  print(f'Answer: {load_text(textfiles[i])}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "bw0Wjpamwspi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Saving**"
      ],
      "metadata": {
        "id": "o4PQixz3qSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Export(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype= tf.string, shape=[])])\n",
        "  def translate(self, inputs):\n",
        "    return self.model.translate(inputs)"
      ],
      "metadata": {
        "id": "cBrd5G6iuhDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = # set save path here\n",
        "export = Export(model)\n",
        "export.translate(audiofiles[1024])\n",
        "tf.saved_model.save(export, SAVE_PATH,\n",
        "                    signatures={'serving_default': export.translate})"
      ],
      "metadata": {
        "id": "WLZJJpO_ptRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}